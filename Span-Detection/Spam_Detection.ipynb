#%% md

# Using Naive Bayes algorithm for spam detection

In this assigment, you will predict if a sms message is 'spam' or 'ham' (i.e. not 'spam') using the Bernoulli Naive Bayes *classifier*.

The training data is from the UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection.  Please download the zip file before running the code below. 


#%% md

## Step 1:  Getting, understanding, and cleaning the dataset


#%% md

###  Importing the dataset

#%%

# Import the usual libraries
import numpy as np 
import pandas as pd  # To read in the dataset we will use the Panda's library
df = pd.read_table('SMSSpamCollection', sep = '\t', header=None, names=['label', 'sms_message'])

# Next we observe the first 5 rows of the data to ensure everything was read correctly
df.head() 

#%% md

### Data pre processing
It would be more convenient if the labels were binary instead of 'ham' and 'spam'.  This way our code can always work with numerical values instead of strings.

#%%

df['label']=df['label'].map({'spam':1, 'ham':0})
df.head() # Again, lets observe the first 5 rows to make sure everything worked before we continue

#%% md

### Splitting the dcoument into training set and test set

#%%

# This time we will use sklearn's method for seperating the data
from sklearn.model_selection import train_test_split

df_train_msgs, df_test_msgs, df_ytrain, df_ytest =\
    train_test_split(df['sms_message'],df['label'], random_state=0)
# print('The Training messages are :')
df_train_msgs.head()

print('The Testing messages are :')
df_test_msgs.head()
print('The training outputs are ' )
df_ytrain.head()
print('The Test outputs are ')
df_ytest.head()
#Looking at the train/test split
print("The number of training examples: ", df_train_msgs.shape[0])
print("The number of test exampels: ", df_test_msgs.shape)

print("The first four labels")
print(df_ytrain[0:4])

print("The first four sms messages")
print(df_train_msgs[0:4])


#%% md

###  Creating the feature vector from the text (feature extraction)

Each message will have its own feature vector.  For each message we will create its feature vector as we discussed in class; we will have a feature for every word in our vocabulary.  The $j$th feature is set to one ($x_j=1$) if the $j$th word from our vocabulary occurs in the message, and set the $j$ feature is set to $0$ otherwise ($x_j=0$).

We will use the sklearn method CountVectorize to create the feature vectors for every messge.

We could have written the code to do this directly by performing the following steps:
* remove capitalization
* remove punctuation 
* tokenize (i.e. split the document into individual words)
* count frequencies of each token 
* remove 'stop words' (these are words that will not help us predict since they occur in most documents, e.g. 'a', 'and', 'the', 'him', 'is' ...

#%%

# importing the library
from sklearn.feature_extraction.text import CountVectorizer
# creating an instance of CountVectorizer
# Note there are issues with the way CountVectorizer removes stop words.  
# To learn more: https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words
vectorizer = CountVectorizer(binary = True, stop_words='english')

# If we wanted to perform Multnomial Naive Bayes, we would include the word counts and use the following code
#vectorizer = CountVectorizer(stop_words='english')

# To see the 'count_vector' object
print(vectorizer)

#%%

# To see the 'stop words' 
print(vectorizer.get_stop_words())


#%%

# Create the vocabulary for our feature transformation\
print('The  df train messages before vect is ' , df_train_msgs)
vectorizer.fit(df_train_msgs)
print('df_train messages are: ', df_train_msgs)
# Next we create the feature vectors for both the training data and the test data
X_train = vectorizer.transform(df_train_msgs).toarray()
# code to turn the training emails into a feature vector
print('X_train is ' , X_train)
# code to turn the test email into a feature vector
X_test = vectorizer.transform(df_test_msgs).toarray() 
print(type(X_test))

# Changing the target vectors data type  
y_train=df_ytrain.to_numpy() 
# Converting from a Panda series to a numpy array
y_test = df_ytest.to_numpy()

# To observe what the data looks like 
print("The label of the first four training examples is: ", y_train[0:4])
print("The first training four examples are: ", X_train[0:4])
# I needed to covert the datatype to list so all the feature 
# values would be printed

#%% md

# Your code goes here

#%%

# You should NOT use the features of sklearn library in your code.
#### TO-DO #####


p_y_1 = len(y_train[y_train==1])/len(y_train)

p_y_0 = len(y_train[y_train==0])/len(y_train)

print('P(y) = 0 ', p_y_0)
print('P (y) == 1 ' , p_y_1)

# print('Vectorizer Vocabulary' , vectorizer.vocabulary_['admirer'])


# Code to get the index of the words admirer and secret from the vectorizer 
# sample_val = vectorizer.transform(['secret'])
# sample_val = vectorizer.transform(['admirer'])
# The index of the sample value is printed as a tuple which can be used ahead
# print(sample_val)

# Obtained 799 as value for token admirer
# and 5642 as the value for token secret 

# Calculating phi(admirerly)
# The array x_train is filtered for those values where the word is present

filtering_variable_phi_ad_0 = y_train[X_train[:,799] ==1] == 0
filtering_variable_phi_ad_1 = y_train[X_train[:,799] ==1] == 1

numerator_phi_ad_0 = np.count_nonzero(filtering_variable_phi_ad_0)
numerator_phi_ad_1 = np.count_nonzero(filtering_variable_phi_ad_1)

# print('The numerator is' , (filtering_variable))
# print((numerator))

# phi_admirerly_0 = ((no of rec where admirer is present (in X_train at index of word admirerly) and y_train is 0 (ie numerator_phi_ad_0 calculated above))) / len(y_train) where y is 0)
# Similar Calculations for phi_admirerly_1
# This is as per the concept of p(x|y) 
phi_admirerly_0 = numerator_phi_ad_0/len(y_train[y_train==0])

phi_admirerly_1 = numerator_phi_ad_1/len(y_train[y_train==1])


print(' Phi_admirerly_0 is ', phi_admirerly_0 )
print(' Phi_admirerly_1 is ', phi_admirerly_1 )


# Calculating phi(secretly)
# The array x_train is filtered for those values where the word is present

filtering_variable_phi_sec_0 = y_train[X_train[:,5642] ==1] == 0
filtering_variable_phi_sec_1 = y_train[X_train[:,5642] ==1] == 1

numerator_phi_sec_0 = np.count_nonzero(filtering_variable_phi_sec_0)
numerator_phi_sec_1 = np.count_nonzero(filtering_variable_phi_sec_1)


# phi_secretly_0 = ((no of rec where secret is present (in X_train at index of word admirerly) and y_train is 0 (ie numerator_0 calculated above))) / len(y_train) where y is 0)
# Similar calculations are done for phi_secretly_1
# This is as per the formula 

phi_secretly_0 = numerator_phi_sec_0/len(y_train[y_train==0])

phi_secretly_1 = numerator_phi_sec_1/len(y_train[y_train==1])


print(' Phi_secretly_0 is ', phi_secretly_0 )
print(' Phi_secretly_1 is ', phi_secretly_1 )


# %%
# Code to get the predict classes of training samples using all the known feature vectors


# The logic would be the same as above =, just that we would run the calculations for all the feature vectors in X_train instead of a single feature vector

############## 

import math

# Calculations for output =0

phi_0_agg = 0 

print('The no of feature vectors are:', X_train.shape[1])


for i in range(0,X_train.shape[1]):
    filtering_variable_phi_0 = y_test[X_test[:,i] ==1] == 0
    numerator_phi_0 = np.count_nonzero(filtering_variable_phi_0)
    phi_0 = numerator_phi_0/len(y_train[y_train==0])
    
    if phi_0 == 0:
        print('The value of phi(i,0) at i =  ' + str(i) 
              + ' is 0 Thus, skipping further calculations '
                'and assigning probability value as 0 for y = 0')
        phi_0_agg = 0
        break
    phi_0_agg = phi_0_agg + math.log(phi_0)
    
# Taking exponent of the aggregated value of phi_0_agg to get its actual value 

# if phi_0_agg != 0:
#     phi_0_agg = math.exp(phi_0_agg)

print('The value of product of probabilities for class 0 is ' , phi_0_agg)

# Calculations for y =1 

phi_1_agg = 0 
for i in range(0,X_train.shape[1]):
    filtering_variable_phi_1 = y_test[X_test[:,i] ==1] == 1
    numerator_phi_1 = np.count_nonzero(filtering_variable_phi_1)
    phi_1 = numerator_phi_1/len(y_train[y_train==1])
    
    if phi_1 == 0:
        print('The value of phi(i,0) at i =  ' + str(i) 
              + ' is 0 Thus, skipping further calculations '
                'and assigning probability value as 0 for y = 1')
        phi_1_agg = 0
        break
    phi_1_agg = phi_1_agg + math.log(phi_1)
    
# Taking exponent of the aggregated value of phi_0_agg to get its actual value 

# if phi_1_agg != 0:
#     phi_1_agg = math.exp(phi_1_agg)
# 
print('The value of product of probabilities for class 1 is ' , phi_1_agg)


#%%

# Code with smoothing on values - when y is 0

# Here, we would perform smoothing using the formula p(x|y) = (t + m)/( N + ms) 
# where t = no of times the value appears in the given data set for that condition on the output variable 
# ie in this case the no of times the feature ( e.g secret is present in the given training set )
# where m is the smoothing factor between 0 to 1
# where N is the total no of values for that condition on output ie no of times when Y is 0 or 1
# where s is number of possible attributes of x(i)




start_list = [0,0,X_train.shape[1]-5]
end_list = [X_train.shape[1],5,X_train.shape[1]]
cases_list = ['All Inputs','First 5 Test Inputs', 'Last 5 Test Inputs']

m = 0.2

for case in range(0,len(cases_list)):
    print('##############################')
    print('Processing for case: ' , cases_list[case])
    phi_1_agg_smooth = 0 
    for i in range(start_list[case],end_list[case]):
        filtering_variable_phi_1_smooth = y_test[X_test[:,i] == 1] == 1
        numerator_phi_1_smooth = np.count_nonzero(filtering_variable_phi_1_smooth)
        phi_1_smooth = (numerator_phi_1_smooth + m)/(len(y_train[y_train == 1]) + m* X_train.shape[1]) 
        
        if phi_1_smooth == 0:
            print('The value of phi(i,0) at i =  ' + str(i) 
                  + ' is 0 Thus, skipping further calculations '
                    'and assigning probability value as 0 for y = 1')
            phi_1_agg_smooth = 0
            break
        # print('The value of phi (i,1) at i = ' + str(i) +' is: ' + str(phi_1_smooth))
        # print('The value of phi_agg_smooth is ' , phi_1_agg_smooth )
        # print('The value of log(phi_smooth is ' , math.log(phi_1_smooth))
        phi_1_agg_smooth = phi_1_agg_smooth + math.log(phi_1_smooth)
        # print('The value of log(phi_agg_smooth is ' ,  math.log(phi_1_agg_smooth))
    
        
    # Taking exponent of the aggregated value of phi_0_agg to get its actual value 
    
    # if phi_1_agg_smooth != 0:
    #     phi_1_agg_smooth = math.exp(phi_1_agg_smooth)
    # 
    print('The value of sum of log of probabilities for class 1 after smoothing is'
          ' ' , phi_1_agg_smooth)



# %%

# Code with smoothing on values - when y is 0

print('\n\n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \n\n')


for case in range(0,len(cases_list)):
    print('##############################')
    print('Processing for case: ' , cases_list[case])
    phi_0_agg_smooth = 0 
    for i in range(start_list[case],end_list[case]):
        filtering_variable_phi_0_smooth = y_test[X_test[:,i] == 1] == 0
        numerator_phi_0_smooth = np.count_nonzero(filtering_variable_phi_0_smooth)
        phi_0_smooth = (numerator_phi_0_smooth + m)/(len(y_train[y_train == 0]) + m* X_train.shape[1]) 
        
        if phi_0_smooth == 0:
            print('The value of phi(i,0) at i =  ' + str(i) 
                  + ' is 0 Thus, skipping further calculations '
                    'and assigning probability value as 0 for y = 0')
            phi_0_agg_smooth = 0
            break
        # print('The value of phi (i,1) at i = ' + str(i) +' is: ' + str(phi_0_smooth))
        # print('The value of log(phi_smooth is ' , math.log(phi_0_smooth))
        phi_0_agg_smooth = phi_0_agg_smooth + math.log(phi_0_smooth)
        # print('The value of phi_agg_smooth is ' , phi_0_agg_smooth)
        # print('The value of log(phi_agg_smooth is ' ,  math.log(phi_1_agg_smooth))

        
    # Taking exponent of the aggregated value of phi_0_agg to get its actual value 
    
    # if phi_1_agg_smooth != 0:
    #     phi_1_agg_smooth = math.exp(phi_1_agg_smooth)
    # 
    print('The value of sum of log of probabilities for class 0 after smoothing is'
          ' ' , phi_0_agg_smooth)
    
    
