#%% md
# Using the K-NN algorithm for classification of iris

In this assigment, you will classify if an Iris is 'Iris Setosa' or 'Iris Versicolour' or 'Iris Virginica' using the k nearest neighbor algorithm.

The training data is from the UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/iris.  Please download the d`ataset before running the code below. 

#%% md

## Step 1:  Getting, understanding, and cleaning the dataset


#%% md

###  Importing the dataset


#%%

# Import the usual libraries
import matplotlib.pyplot as plt # plotting utilities 
%matplotlib inline
import numpy as np 
import pandas as pd  # To read in the dataset we will use the Panda's library
df = pd.read_csv('iris.csv', header=None, names = ["sepal length[cm]","sepal width[cm]","petal length[cm]", "petal width", "label"])

# Next we observe the first 5 rows of the data to ensure everything was read correctly
df.head()



#%% md

### Data preprocesssing
It would be more convenient if the labels were integers instead of 'Iris-setosa', 'Iris-versicolor' and 'Iris-virginica'.  This way our code can always work with numerical values instead of strings.

#%%

df['label'] = df.label.map({'Iris-setosa': 0,
              'Iris-versicolor': 1,
              'Iris-virginica': 2})
df.head()# Again, lets observe the first 5 rows to make sure everything worked before we continue

#%%

# This time we will use sklearn's method for seperating the data
from sklearn.model_selection import train_test_split
names = ["sepal length[cm]","petal width"]
#After completing the assignment, try your code with all the features
#names = ["sepal length[cm]","sepal width[cm]","petal length[cm]", "petal width"]
df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df[names],df['label'], 
                                                                random_state=0)
X_train=df_X_train.to_numpy()
X_test=df_X_test.to_numpy()
y_train=df_y_train.to_numpy()
y_test=df_y_test.to_numpy()

#Looking at the train/test split
print("The number of training examples: ", X_train.shape[0])
print("The number of test examples: ", X_test.shape[0])

print("The first four training labels")
print(y_train[0:4])

print("The first four iris' measurements")
print(X_test[0:4])

#%% md

## visualizing the data set

Using a scatter plot to visualize the dataset

#%%

iris_names=['Iris-setosa','Iris-versicolor','Iris-virginica']
for i in range(0,3):
    plt.scatter(X_train[y_train == i, 0],
                X_train[y_train == i, 1],
            marker='o',
            label='class '+ str(i)+ ' '+ iris_names[i])

plt.xlabel('sepal width[cm]')
plt.ylabel('petal length[cm]')
plt.legend(loc='lower right')

plt.show()

#%% md

# Your code goes here

#%%

import math
def euclidean_distance(x1, x2):
  #### TO-DO #####      
    # The Euclidean distance is calculated between x1 and x2 for all the features it has
    # For n features, it would be the square root of the sum of squares of differences between the corresponding features
    euclidean_dist = 0
    # print('x1 is ' , x1)
    # print('x2 is ' , x2)
    for k in range(0, len(x1)):
        euclidean_dist = euclidean_dist + (x1[k]-x2[k])*(x1[k]-x2[k])
    euclidean_dist = math.sqrt(euclidean_dist)
    # print('The Euclidean distance is:',euclidean_dist )
    return  euclidean_dist
  ##############
  
def manhattan_distance(x1,x2):
    manhattan_dist = 0
    # print('x1 is ' , x1)
    # print('x2 is ' , x2)
    for k in range(0, len(x1)):
        manhattan_dist = manhattan_dist + (abs(x1[k]- x2[k]))
    # print('The Manhattan distance is:',manhattan_dist)
    return  manhattan_dist


def get_class_of_neighbors( X, y, x_test, k, distance = 'euclidean_distance'):
  #### TO-DO ##### 
    # Here, we get the k nearest neighbours of the Vector X as per euclidean distance using output vector y 
    try:
        
        # print('X is ' , X)
        # print('y is ' , y)
        # print('x_test is  ',x_test)
        # print('Shape of x_test is ' , x_test.shape[0])
        # print('k is ', k)
        distance_list = np.zeros(shape=(x_test.shape[0],2))
        for j in range(0,x_test.shape[0]):
            if distance == 'euclidean_distance':
                distance_list[j,0] = euclidean_distance(x1= X,x2= x_test[j,:])
            elif distance == 'manhattan_distance':
                distance_list[j,0] = manhattan_distance(x1 =X, x2 = x_test[j,:])
            distance_list[j,1]  = y[j]
        neighbors = distance_list[distance_list[:,0].argsort()]
        # print('The complete list of neighbors is' , neighbors)
        neighbors = neighbors[0:k]
        # print('The classes of neighbors are ' , neighbors[:,1])
      ##############
        return neighbors[:,1]
    except Exception as e:
        print('Error occurred in function get_class_of_neighbors with Exception ' , str(e))

#%%

  #### TO-DO ######
def perform_knn(input_vector,output_vector,training_input_vector,training_output_vector,k_list,distance_list):
    try:
        print('###########################################################')
        print('Starting Processing ')
        if isinstance(k_list,list) == False:
            raise Exception('Argument k_list should be of type list even if it is a single element ')
        if isinstance(distance_list,list) == False:
                    raise Exception('Argument distance_list should be of type list even if it is a single element ')
        for distance in list_of_distances:
            print('Using Distance ' , distance , ' in this iteration')
            for k in k_list:
                print('Carrying out K Means Clustering for k = ', k)
                print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \n ') 
                yout=[0]*training_input_vector.shape[0]
                incorrectly_classified_samples= np.zeros(shape=(training_input_vector.shape[0],
                                                                training_input_vector.shape[1]))
                # print('Input vector is ', input_vector)
                # print('Training Vector with inputs is ', training_input_vector)
                for t in range(0,input_vector.shape[0]):
                    # neighbors_class =  get_class_of_neighbors(input_vector=training_input_vector[t,:],y=training_output_vector,training_input_vector=training_input_vector,k=k)
                    
                    neighbors_class =  get_class_of_neighbors(X = input_vector[t,:], y = training_output_vector,
                                                              x_test = training_input_vector,k = k,distance = distance)
                    # print('The neighbors class is ' , neighbors_class)
                    final_class = 0
                    count_earlier =0
                    for l in range(0,3):
                        count = np.count_nonzero(neighbors_class == l)
                        # print('L is ', l)
                        # print('The count is ' , count)
                        if count > count_earlier:
                            final_class=l
                            count_earlier = count   
                    yout[t] = final_class
                # print('Yout is ', yout )                  
                count_incorrectly_classified =0
                for i in range(0,output_vector.shape[0]):
                    if yout[i] == output_vector[i]:
                        # print('Test Input ' + str(input_vector[i,:])+  'was correctly classified') 
                        pass
                    else: 
                        # print('Test Input ' + str(input_vector[i,:])+  'was incorrectly classified')
                        count_incorrectly_classified = count_incorrectly_classified+1
                        # incorrectly_classified_samples[i,:]=training_input_vector[i,:]
                # print('The count of incorrectly classified samples is:' , count_incorrectly_classified )
                accuracy = (len(output_vector) - count_incorrectly_classified)/len(output_vector)
                print('The accuracy for k = ' ,k ,' is ' , accuracy )
    except Exception as e:
        print('Error occurred while processing due to exception' + str(e))

#%%

    # Code for Zero R Classifier
    class_zero_classifier = 0
    earlier_count_zero_classifier = 0
    count_zero_classifier=0
    for i in range(0,3):
        count_zero_classifier = np.count_nonzero(y_test==i)   
        if count_zero_classifier > earlier_count_zero_classifier:
            class_zero_classifier = i            
    y_zero_classifier = [class_zero_classifier] *len(y_test)
    print('The Output of Zero-R classifier is :' , y_zero_classifier)
    print('The output of test samples is:' , y_test)
    # Calculating Accuracy 
    count_incorrectly_classified =0
    for i in range(0, y_test.shape[0]):
        # print('Y Out' ,yout[i])
        # print('Y Test ',training_output_vector[i])
        if y_zero_classifier[i] != y_test[i]:
            # print('Test Input ' + str(testing_input_vector[i,:])+  'was correctly classified') 
            # print('Test Input ' + str(X_test[i,:])+  'was incorrectly classified')
            count_incorrectly_classified = count_incorrectly_classified+1
        else: 
            pass
            # incorrectly_classified_samples[i,:]=training_input_vector[i,:]
    print('The count of incorrectly classified samples is:' , count_incorrectly_classified )
    accuracy = (len(y_test) - count_incorrectly_classified)/len(y_test)
    print('The accuracy for Zero R classifier is  '  , accuracy )

#%%

# Applying KNN

list_of_k = [1,3,5]
list_of_distances = ['euclidean_distance', 'manhattan_distance'] 

perform_knn(input_vector=X_test,output_vector=y_test,training_input_vector=X_train,
            training_output_vector=y_train,k_list=list_of_k,distance_list=list_of_distances)


# Applying 5 fold validation 

# division = math.ceil(X_train.shape[0]/5)
# print('The division is ' , division)
# # perform_knn(input_vector=X_test,output_vector=y_test,training_input_vector=X_train,
# #             training_output_vector=y_train,k_list=list_of_k,distance_list=list_of_distances)
