#%% md

# Salmon classification with the bivariate Gaussian

In this assigment, you will predict if a fish is an 'Alaskan' salmon or a 'Canadian' salmon.

The algorithm you will use a generative algorithm.  Where you model each class as a **bivariate Gaussian**.

#%% md

## Step 0. Import statements

The Python programming language, as most programming languages, is augmented by **modules**.  These modules contain functions and classes for specialized tasks needed in machine learning.

Below, we will `import` three modules:
* **pandas**
* **numpy**
* **matplotlib.pyplot**

Note that we imported these modules using **aliases**

#%%

# Standard libraries
%matplotlib inline
import pandas as pd
import numpy as np # for better array operations
import matplotlib.pyplot as plt # plotting utilities 
import math
# Module computing the Gaussian density
from scipy.stats import norm, multivariate_normal 


#%% md

## Step 1. Data preparation: loading, understanding and cleaning the dataset

#%% md

### Importing the dataset
Make sure the file `Salmon_dataset.csv` is in the same directory as this notebook.

The dataset contains 100  examples, each example has 3 features (*gender, Freshwater, marine*) and a label (*Alaskan, Canadian*).

#%%

# Loading the data set using Panda's in a dataframe 

df = pd.read_csv('Salmon_dataset.csv', delimiter=',') 

#Lets check that everything worked before continuing on
df.head()

#%% md

### Data preprocesssing
We will change the labels 'Alaskan' and 'Canadian' to $0$ and $1$ respectively.  In our code it is easier to work with numerical values instead of strings.

Often we will do more dataprepocessing, such as looking for missing values and scaling the data though that is NOT required for this assignment yet. 

#%%

# It is easier to work with the data if the labels are integers
# Changing the 'Origin' column values, map 'Alaskan':0 and 'Canadian':1
df['Origin']=df.Origin.map({'Alaskan':0, 'Canadian':1})

#Lets check that everything worked before continuing on

print(df.head(30))

#%%

# We will store the dataframe as a Numpy array
data = df.to_numpy() 

# Split the examples into a training set (trainx, trainy) and test set (testx, testy) 
########## TO DO ##########
n =  data.shape[0] # the number of rows
train_n = int(.9*n) # this test set is a bit small to really evaluate our hypothesis - what could we do to get a better estimate and still keep most of the data to estimate our parameters?
np.random.seed(0) 
# Our code randomly chooses which examples will be the training data, but for grading purposes we want the random numbers 
# used to seperate the data are the same for everyone
perm = np.random.permutation(n)
trainx = data[perm[0:train_n],1:3] #selecting the two of the features `Freshwater' and 'Marine'
trainy = data[perm[0:train_n],3]           
testx = data[perm[0:train_n:n], 1:3] # We won't look at the testx data until it is time to evauate our hypothesis.  This numpy array contains the set of test data for the assignment
testy = data[perm[train_n:n],3]

##########


#%% md

### Plotting the dataset
Visualization can be helpful when exploring and getting to know a dataset.

#%%

# plotting the Alaskan salmon as a green dot
plt.plot(trainx[trainy==0,0], trainx[trainy==0,1], marker='o', ls='None', c='g')
# plotting the Canadian salmon as a red dot
plt.plot(trainx[trainy==1,0], trainx[trainy==1,1], marker='o', ls='None', c='r')

#%% md

## Step 2. Model training: implementing Gaussian Discriminant Analysis

#%% md

###  Sufficient statistics

Just as if we were doing these calculations by hand, we break the process down into manageable pieces

Our first helper function will find the mean and covariance of the Gaussian for a set of examples

#%%

# Input: a design matrix
# Output: a numpy array containing the means for each feature, and a 2-dimensional numpy array containing the covariance matrix sigma
def fit_gaussian(x, input_type=None):
    # Code for mean (mu) 
    # Currently code assumes x is a dataframe that is filtered for the given class instead of the numpy dataframe constructed above    
    # The features to be used are hardcoded here as a list. 
    # The code is written such that it calculates mean for the features in the given list. 
    # This list can be taken as a function argument so that the code becomes generic
    feature_list = ['Freshwater', 'Marine']
    # Initialized numpy array of zeros
    mu = np.zeros(shape=(len(feature_list), 1))
    sigma = np.zeros(shape=(len(feature_list), 2))
    
    # counter initialized fot looping 
    i = 0
    for feature in feature_list:
           # Filtering steps to get only one class input at a time, ( To be done by the caller function
               #filtered_df = df['Origin'] == 0
               #df_for_processing = df[x]
               # print('df for pre processing is' ,df_for_processing)
           mean_processing_df = x[feature]
               # print('df for pre processing is', df_for_processing)
           # End of filtering steps in caller function
           # print('Sum is',sum(df_for_processing))
           # print('Len is' ,len(df_for_processing))
           mu[i, 0] = sum(mean_processing_df)/len(mean_processing_df)
           i = i + 1
    # Check statements
    #print(mu)
    #print('Type of mu is ' , type(mu))

    # Code for getting covariance matrix
    # Generalized to work for any no of features for a given input class matrix
    # Can be optimized by removing else condition and not allowing loop variable to go ahead in case i > j
    for i in range(0, len(feature_list)):
        for j in range(0,len(feature_list)):
            if input_type == 'Canadian':
                for k in range(50,len(x)+50):
                    if i < j or i == j:
                        sigma[i, j] = sigma[i, j] + (x[feature_list[i]][k] - mu[i, 0]) * (x[feature_list[j]][k] - mu[i ,0])
                    else:
                        # Using the symmetry of covariance matrix to avoid calculations
                        sigma[i,j] = sigma[j,i]
            else:
                    for k in range(0,len(x)):
                        if i < j or i == j:
                            sigma[i, j] = sigma[i, j] + (x[feature_list[i]][k] - mu[i, 0]) * (x[feature_list[j]][k] - mu[i ,0])
                        else:
                            # Using the symmetry of covariance matrix to avoid calculations
                            sigma[i,j] = sigma[j,i]
    sigma = sigma/len(x)
    # Check statements
    #print('Sigma is: ', sigma)
    #print('Type of sigma is ', type(sigma))
##########
    return mu, sigma

#%% md


Before moving on, test your code to make sure it works correctly.  

Solution:
Tested Code by giving input file and calculated its mean and covariance matrix values on Excel sheet



df = pd.read_csv('Salmon_dataset.csv', delimiter=',')
df['Origin']=df.Origin.map({'Alaskan':0, 'Canadian':1})
feature_list = ['Freshwater', 'Marine']
Test for Mu ( for class Alaskan)
#########################################################
mu = np.zeros(shape=(len(feature_list), 1))
sigma = np.zeros(shape=(len(feature_list), len(feature_list)))
i = 0
for feature in feature_list:
       filtered_df = df['Origin'] == 0
       df_for_processing = df[filtered_df]
       # print('df for pre processing is' ,df_for_processing)
       df_for_processing = df_for_processing[feature]
       # print('df for pre processing is', df_for_processing)
       # print('Sum is',sum(df_for_processing))
       # print('Len is' ,len(df_for_processing))
       mu[i, 0] = sum(df_for_processing)/len(df_for_processing)
       i = i + 1
print(mu)
print('Type of mu is ' , type(mu))

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

OUTPUTS:
Output value for mu:
[[ 98.38]
 [429.66]]
 
 Output for type(mu):
 Type of mu is  <class 'numpy.ndarray'>
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
######################################################################

Test for Sigma ( for class Alaskan)
######################################################################

filtered_df = df['Origin'] == 0
df_for_processing = df[filtered_df]
df_for_processing = df_for_processing[feature_list]
print('Pre Processing DF is ' , df_for_processing)
# mu[0,0] = 99
# mu[1,0] = 429

for i in range(0, len(feature_list)):
    for j in range(0,len(feature_list)):
        for k in range(0,len(df_for_processing)):
            if i <j or i ==j:
                # print('I ' ,i)
                # print('J',j)
                # print('K',k)
                # print('Sigma', sigma)
                sigma[i, j] = sigma[i, j] + (df_for_processing[feature_list[i]][k] - mu[i, 0]) * (df_for_processing[feature_list[j]][k] - mu[i ,0])
            else:
                # Using the symmetry of covariance matrix to avoid calculations
                sigma[i,j] = sigma[j,i]

sigma = sigma/len(df_for_processing)

print('Type of sigma is ', type(sigma))
print(sigma)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
OUTPUTS
Output value for sigma:
[[ 255.3956 -184.3308]
 [-184.3308 1371.1044]]

Output for type of sigma
Type of sigma is  <class 'numpy.ndarray'>
######################################################################

#%% md

### Write the rest of your code here






#%%

# Inputs x: A numpy array  vector of input values for the respective features
#        mu: ( A 1 * d) numpy array of mean of  each the feature vectors
#        sigma: ( An Nd) numpy array of the covariance of the feature vectors
def calc_probability(x,mu,sigma):

    sqrt_det_sigma = 1/math.sqrt(np.linalg.det(sigma))
    pi_root_constant = 1/(math.sqrt(2 * math.pi))
    normalization_constant = sqrt_det_sigma*pi_root_constant
    # print('Normalization constant  is' , normalization_constant)
    inp_vec_sub_mu = x - mu
    inp_vec_sub_mu_transpose = np.transpose(inp_vec_sub_mu)
    sigma_inverse = np.linalg.inv(sigma)
    prod_for_exp_1 =np.matmul(inp_vec_sub_mu_transpose,sigma_inverse)
    exp_value = np.matmul(prod_for_exp_1,inp_vec_sub_mu)
    exp_value = -1*exp_value/2    
    # print("Exp Value is " , exp_value)
    # print(" Its Shape is:" , exp_value.shape)
    exponent = math.exp(exp_value)
    probability_value = normalization_constant*exponent
    return probability_value
    
# Getting the mu and sigma for Alaskan class


filtered_alaskan_df = df['Origin'] == 0
alaskan_df = df[filtered_alaskan_df]

mu_alaskan, sigma_alaskan = fit_gaussian(alaskan_df,input_type=None)

# Print Checks 
print('The mu for alaskan is ' , mu_alaskan)
print('The sigma for alaskan is ' , sigma_alaskan)

filtered_canadian_df = df['Origin'] == 1
canadian_df = df[filtered_canadian_df]
# canadian_df.set_index(canadian_df.columns
# canadian_df.reset_index(inplace = True)
# index_counter = 0
# for key in canadian_df['Freshwater'].keys():
#     print('The Key is ' , key)
#     canadian_df.rename(index={key:index_counter})
#     index_counter = index_counter + 1
# print('Canadian DF is ' , canadian_df) 

mu_canadian ,sigma_canadian = fit_gaussian(canadian_df, input_type = 'Canadian')

print('The mu for canadian is ' , mu_canadian)
print('The sigma for canadian is ' , sigma_canadian)
#############################################################################
# Calculate the probabilities of each value in the input data set and predict the class for which probability is higher  

# Preparing an input vector 

input_vector_for_prediction = np.array([[144,403],
                               [76,442],
                               [100,470],
                               [155,349],
                               [99,403],
                               [124,341],
                               [136,348],
                               [152,301],
                               [99,481],
                               [80,398]])
print('Input Vector is ', input_vector_for_prediction )
print('The Shape of input Vector is ', input_vector_for_prediction.shape)

# Please note: This part of the program is not working correctly due to dimension error in the input vector 
# The function calc_probability was tested for an input array of 2*1 however I am not able to replicate the same here and thus am not able to predict the classes 
predicted_list = list()
for x in input_vector_for_prediction:
    x = x.reshape(1, input_vector_for_prediction.shape[1])
    # print(type(x))
    alaskan_probability = calc_probability(x=x.T,mu=mu_alaskan,sigma=sigma_alaskan)
    # print('The Alaskan probability is ' ,alaskan_probability)
    canadian_probability = calc_probability(x=x.T,mu=mu_canadian,sigma=sigma_canadian)
    # print('The Canadian probability is ' ,canadian_probability)
    if alaskan_probability > canadian_probability:
        predicted_list.append("Alaskan")
    else:
        predicted_list.append("Canadian")

print(" The predicted classes for the samples are:" , predicted_list)




#%% md

# Salmon classification with the bivariate Gaussian

In this assigment, you will predict if a fish is an 'Alaskan' salmon or a 'Canadian' salmon.

The algorithm you will use a generative algorithm.  Where you model each class as a **bivariate Gaussian**.

#%% md

## Step 0. Import statements

The Python programming language, as most programming languages, is augmented by **modules**.  These modules contain functions and classes for specialized tasks needed in machine learning.

Below, we will `import` three modules:
* **pandas**
* **numpy**
* **matplotlib.pyplot**

Note that we imported these modules using **aliases**

#%%

# Standard libraries
%matplotlib inline
import pandas as pd
import numpy as np # for better array operations
import matplotlib.pyplot as plt # plotting utilities 
import math
# Module computing the Gaussian density
from scipy.stats import norm, multivariate_normal 


#%% md

## Step 1. Data preparation: loading, understanding and cleaning the dataset

#%% md

### Importing the dataset
Make sure the file `Salmon_dataset.csv` is in the same directory as this notebook.

The dataset contains 100  examples, each example has 3 features (*gender, Freshwater, marine*) and a label (*Alaskan, Canadian*).

#%%

# Loading the data set using Panda's in a dataframe 

df = pd.read_csv('Salmon_dataset.csv', delimiter=',') 

#Lets check that everything worked before continuing on
df.head()

#%% md

### Data preprocesssing
We will change the labels 'Alaskan' and 'Canadian' to $0$ and $1$ respectively.  In our code it is easier to work with numerical values instead of strings.

Often we will do more dataprepocessing, such as looking for missing values and scaling the data though that is NOT required for this assignment yet. 

#%%

# It is easier to work with the data if the labels are integers
# Changing the 'Origin' column values, map 'Alaskan':0 and 'Canadian':1
df['Origin']=df.Origin.map({'Alaskan':0, 'Canadian':1})

#Lets check that everything worked before continuing on

print(df.head(30))

#%%

# We will store the dataframe as a Numpy array
data = df.to_numpy() 

# Split the examples into a training set (trainx, trainy) and test set (testx, testy) 
########## TO DO ##########
n =  data.shape[0] # the number of rows
train_n = int(.9*n) # this test set is a bit small to really evaluate our hypothesis - what could we do to get a better estimate and still keep most of the data to estimate our parameters?
np.random.seed(0) 
# Our code randomly chooses which examples will be the training data, but for grading purposes we want the random numbers 
# used to seperate the data are the same for everyone
perm = np.random.permutation(n)
trainx = data[perm[0:train_n],1:3] #selecting the two of the features `Freshwater' and 'Marine'
trainy = data[perm[0:train_n],3]           
testx = data[perm[0:train_n:n], 1:3] # We won't look at the testx data until it is time to evauate our hypothesis.  This numpy array contains the set of test data for the assignment
testy = data[perm[train_n:n],3]

##########


#%% md

### Plotting the dataset
Visualization can be helpful when exploring and getting to know a dataset.

#%%

# plotting the Alaskan salmon as a green dot
plt.plot(trainx[trainy==0,0], trainx[trainy==0,1], marker='o', ls='None', c='g')
# plotting the Canadian salmon as a red dot
plt.plot(trainx[trainy==1,0], trainx[trainy==1,1], marker='o', ls='None', c='r')

#%% md

## Step 2. Model training: implementing Gaussian Discriminant Analysis

#%% md

###  Sufficient statistics

Just as if we were doing these calculations by hand, we break the process down into manageable pieces

Our first helper function will find the mean and covariance of the Gaussian for a set of examples

#%%

# Input: a design matrix
# Output: a numpy array containing the means for each feature, and a 2-dimensional numpy array containing the covariance matrix sigma
def fit_gaussian(x, input_type=None):
    # Code for mean (mu) 
    # Currently code assumes x is a dataframe that is filtered for the given class instead of the numpy dataframe constructed above    
    # The features to be used are hardcoded here as a list. 
    # The code is written such that it calculates mean for the features in the given list. 
    # This list can be taken as a function argument so that the code becomes generic
    feature_list = ['Freshwater', 'Marine']
    # Initialized numpy array of zeros
    mu = np.zeros(shape=(len(feature_list), 1))
    sigma = np.zeros(shape=(len(feature_list), 2))
    
    # counter initialized fot looping 
    i = 0
    for feature in feature_list:
           # Filtering steps to get only one class input at a time, ( To be done by the caller function
               #filtered_df = df['Origin'] == 0
               #df_for_processing = df[x]
               # print('df for pre processing is' ,df_for_processing)
           mean_processing_df = x[feature]
               # print('df for pre processing is', df_for_processing)
           # End of filtering steps in caller function
           # print('Sum is',sum(df_for_processing))
           # print('Len is' ,len(df_for_processing))
           mu[i, 0] = sum(mean_processing_df)/len(mean_processing_df)
           i = i + 1
    # Check statements
    #print(mu)
    #print('Type of mu is ' , type(mu))

    # Code for getting covariance matrix
    # Generalized to work for any no of features for a given input class matrix
    # Can be optimized by removing else condition and not allowing loop variable to go ahead in case i > j
    for i in range(0, len(feature_list)):
        for j in range(0,len(feature_list)):
            if input_type == 'Canadian':
                for k in range(50,len(x)+50):
                    if i < j or i == j:
                        sigma[i, j] = sigma[i, j] + (x[feature_list[i]][k] - mu[i, 0]) * (x[feature_list[j]][k] - mu[i ,0])
                    else:
                        # Using the symmetry of covariance matrix to avoid calculations
                        sigma[i,j] = sigma[j,i]
            else:
                    for k in range(0,len(x)):
                        if i < j or i == j:
                            sigma[i, j] = sigma[i, j] + (x[feature_list[i]][k] - mu[i, 0]) * (x[feature_list[j]][k] - mu[i ,0])
                        else:
                            # Using the symmetry of covariance matrix to avoid calculations
                            sigma[i,j] = sigma[j,i]
    sigma = sigma/len(x)
    # Check statements
    #print('Sigma is: ', sigma)
    #print('Type of sigma is ', type(sigma))
##########
    return mu, sigma

#%% md


Before moving on, test your code to make sure it works correctly.  

Solution:
Tested Code by giving input file and calculated its mean and covariance matrix values on Excel sheet



df = pd.read_csv('Salmon_dataset.csv', delimiter=',')
df['Origin']=df.Origin.map({'Alaskan':0, 'Canadian':1})
feature_list = ['Freshwater', 'Marine']
Test for Mu ( for class Alaskan)
#########################################################
mu = np.zeros(shape=(len(feature_list), 1))
sigma = np.zeros(shape=(len(feature_list), len(feature_list)))
i = 0
for feature in feature_list:
       filtered_df = df['Origin'] == 0
       df_for_processing = df[filtered_df]
       # print('df for pre processing is' ,df_for_processing)
       df_for_processing = df_for_processing[feature]
       # print('df for pre processing is', df_for_processing)
       # print('Sum is',sum(df_for_processing))
       # print('Len is' ,len(df_for_processing))
       mu[i, 0] = sum(df_for_processing)/len(df_for_processing)
       i = i + 1
print(mu)
print('Type of mu is ' , type(mu))

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

OUTPUTS:
Output value for mu:
[[ 98.38]
 [429.66]]
 
 Output for type(mu):
 Type of mu is  <class 'numpy.ndarray'>
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
######################################################################

Test for Sigma ( for class Alaskan)
######################################################################

filtered_df = df['Origin'] == 0
df_for_processing = df[filtered_df]
df_for_processing = df_for_processing[feature_list]
print('Pre Processing DF is ' , df_for_processing)
# mu[0,0] = 99
# mu[1,0] = 429

for i in range(0, len(feature_list)):
    for j in range(0,len(feature_list)):
        for k in range(0,len(df_for_processing)):
            if i <j or i ==j:
                # print('I ' ,i)
                # print('J',j)
                # print('K',k)
                # print('Sigma', sigma)
                sigma[i, j] = sigma[i, j] + (df_for_processing[feature_list[i]][k] - mu[i, 0]) * (df_for_processing[feature_list[j]][k] - mu[i ,0])
            else:
                # Using the symmetry of covariance matrix to avoid calculations
                sigma[i,j] = sigma[j,i]

sigma = sigma/len(df_for_processing)

print('Type of sigma is ', type(sigma))
print(sigma)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
OUTPUTS
Output value for sigma:
[[ 255.3956 -184.3308]
 [-184.3308 1371.1044]]

Output for type of sigma
Type of sigma is  <class 'numpy.ndarray'>
######################################################################

#%% md

### Write the rest of your code here






#%%

# Inputs x: A numpy array  vector of input values for the respective features
#        mu: ( A 1 * d) numpy array of mean of  each the feature vectors
#        sigma: ( An Nd) numpy array of the covariance of the feature vectors
def calc_probability(x,mu,sigma):

    sqrt_det_sigma = 1/math.sqrt(np.linalg.det(sigma))
    pi_root_constant = 1/(math.sqrt(2 * math.pi))
    normalization_constant = sqrt_det_sigma*pi_root_constant
    # print('Normalization constant  is' , normalization_constant)
    inp_vec_sub_mu = x - mu
    inp_vec_sub_mu_transpose = np.transpose(inp_vec_sub_mu)
    sigma_inverse = np.linalg.inv(sigma)
    prod_for_exp_1 =np.matmul(inp_vec_sub_mu_transpose,sigma_inverse)
    exp_value = np.matmul(prod_for_exp_1,inp_vec_sub_mu)
    exp_value = -1*exp_value/2    
    # print("Exp Value is " , exp_value)
    # print(" Its Shape is:" , exp_value.shape)
    exponent = math.exp(exp_value)
    probability_value = normalization_constant*exponent
    return probability_value
    
# Getting the mu and sigma for Alaskan class


filtered_alaskan_df = df['Origin'] == 0
alaskan_df = df[filtered_alaskan_df]

mu_alaskan, sigma_alaskan = fit_gaussian(alaskan_df,input_type=None)

# Print Checks 
print('The mu for alaskan is ' , mu_alaskan)
print('The sigma for alaskan is ' , sigma_alaskan)

filtered_canadian_df = df['Origin'] == 1
canadian_df = df[filtered_canadian_df]
# canadian_df.set_index(canadian_df.columns
# canadian_df.reset_index(inplace = True)
# index_counter = 0
# for key in canadian_df['Freshwater'].keys():
#     print('The Key is ' , key)
#     canadian_df.rename(index={key:index_counter})
#     index_counter = index_counter + 1
# print('Canadian DF is ' , canadian_df) 

mu_canadian ,sigma_canadian = fit_gaussian(canadian_df, input_type = 'Canadian')

print('The mu for canadian is ' , mu_canadian)
print('The sigma for canadian is ' , sigma_canadian)
#############################################################################
# Calculate the probabilities of each value in the input data set and predict the class for which probability is higher  

# Preparing an input vector 

input_vector_for_prediction = np.array([[144,403],
                               [76,442],
                               [100,470],
                               [155,349],
                               [99,403],
                               [124,341],
                               [136,348],
                               [152,301],
                               [99,481],
                               [80,398]])
print('Input Vector is ', input_vector_for_prediction )
print('The Shape of input Vector is ', input_vector_for_prediction.shape)

# Please note: This part of the program is not working correctly due to dimension error in the input vector 
# The function calc_probability was tested for an input array of 2*1 however I am not able to replicate the same here and thus am not able to predict the classes 
predicted_list = list()
for x in input_vector_for_prediction:
    x = x.reshape(1, input_vector_for_prediction.shape[1])
    # print(type(x))
    alaskan_probability = calc_probability(x=x.T,mu=mu_alaskan,sigma=sigma_alaskan)
    # print('The Alaskan probability is ' ,alaskan_probability)
    canadian_probability = calc_probability(x=x.T,mu=mu_canadian,sigma=sigma_canadian)
    # print('The Canadian probability is ' ,canadian_probability)
    if alaskan_probability > canadian_probability:
        predicted_list.append("Alaskan")
    else:
        predicted_list.append("Canadian")

print(" The predicted classes for the samples are:" , predicted_list)




