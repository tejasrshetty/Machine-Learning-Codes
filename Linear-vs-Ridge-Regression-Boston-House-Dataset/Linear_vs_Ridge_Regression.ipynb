    #%% md

# Programming Assignment 3 - Simple Linear versus Ridge Regression 

#%% md

Your friend Bob just moved to Boston. He is a real estate agent who is trying to evaluate the prices of houses in the Boston area. He has been using a linear regression model but he wonders if he can improve his accuracy on predicting the prices for new houses. He comes to you for help as he knows that you're an expert in machine learning. 

As a pro, you suggest doing a *polynomial transformation*  to create a more flexible model, and performing ridge regression since having so many features compared to data points increases the variance. 

Bob, however, being a skeptic isn't convinced. He wants you to write a program that illustrates the difference in training and test costs for both linear and ridge regression on the same dataset. Being a good friend, you oblige and hence this assignment :) 

#%% md

In this notebook, you are to explore the effects of ridge regression.  We will use a dataset that is part of the sklearn.dataset package.  Learn more at https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html

#%% md

## Step 1:  Getting, understanding, and preprocessing the dataset

We first import the standard libaries and some libraries that will help us scale the data and perform some "feature engineering" by transforming the data into $\Phi_2({\bf x})$

#%%

import numpy as np
import sklearn
from sklearn.datasets import load_boston
from sklearn.preprocessing import PolynomialFeatures
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import sklearn.linear_model
from sklearn.model_selection import KFold

#%% md

###  Importing the dataset

#%%

# Import the boston dataset from sklearn
boston_data = load_boston()

#%%

#  Create X and Y variables - X holding the .data and Y holding .target 
X = boston_data.data
y = boston_data.target

#  Reshape Y to be a rank 2 matrix 
y = y.reshape(X.shape[0], 1)

# Observe the number of features and the number of labels
print('The number of features is: ', X.shape[1])
# Printing out the features
print('The features: ', boston_data.feature_names)
# The number of examples
print('The number of examples in our dataset: ', X.shape[0])
#Observing the first 2 rows of the data
print(X[0:2])


#%% md

We will also create polynomial features for the dataset to test linear 
and ridge regression on data with d = 1 and data with d = 2. 
Feel free to increase the # of degress and see what effect it has on the training and test error. 

#%%

# Create a PolynomialFeatures object with degree = 2. 
# Transform X and save it into X_2. Simply copy Y into Y_2 
poly = PolynomialFeatures(degree=2)
X_2 = poly.fit_transform(X)
y_2 = y

#%%

# the shape of X_2 and Y_2 - should be (506, 105) and (506, 1) respectively
print(X_2.shape)
print(y_2.shape)

#%% md

# Your code goes here

#%%

# Calculating the pseudoinverse of X and multiplying with y to get the weight matrix

# TODO - Define the get_coeff_ridge_normaleq function. Use the normal equation method.
# TODO - Return w values

def get_coeff_lin_reg(X_train,y_train):
    pseudoinv_X_train = np.linalg.pinv(X_train)
    w_lin = np.matmul(pseudoinv_X_train,y_train)
    return w_lin

def get_coeff_ridge_normaleq(X_train, y_train, alpha):
    # use np.linalg.pinv(a)
    # The closed form solution for w in ridge regression is 
    # w = [Xt*y]inv([Xt*X + lambda*I))
    #### TO-DO #####
    
    
    # Creating Identity matrix = length of X_train
    I = np.identity(X_train.shape[1]) 
    term_1 = alpha*I
    term_2 = np.matmul(np.transpose(X_train),X_train)
    # print('Tern 1 is: ' , term_1 )
    # print('Tern 2 is ' , term_2)
    # print('Shape of X_Train  is', X_train.shape)
    # print('Shape of y_train is ', y_train.shape )
    first_multiplier_term = np.linalg.inv(term_1 + term_2)
    # print('Shape of first Multiplier term is ' , first_multiplier_term.shape)    
    second_multiplier_term = np.matmul(np.transpose(X_train) ,y_train)
    # print('Done')
    # print('Shape of Second Multiplier term is ' , second_multiplier_term.shape)
    w = np.matmul(first_multiplier_term,second_multiplier_term)
    ##############
    return w

#%%

# TODO - Define the evaluate_err_ridge function.
# TODO - Return the train_error and test_error values


# def evaluate_err(X_train, X_test, y_train, y_test, w): 
#     #### TO-DO #####
#     
#     ##############
#     return train_error, test_error

#%%

# TODO - Finish writing the k_fold_cross_validation function. 
# TODO - Returns the average training error and average test error from the k-fold cross validation
# use Sklearns K-Folds cross-validator: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html

def k_fold_cross_validation(k, X, y,type_test,alpha=None):
    kf = KFold(n_splits=k, random_state=21, shuffle=True)
    total_E_val_test = 0
    total_E_val_train = 0
    E_train_list =list()
    E_test_list =list()
    w_lin = np.zeros(shape=(13,1))
    w_ridge = np.zeros(shape=(13,1))
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        w_lin = 0
        w_ridge = 0
        # centering the data so we do not need the intercept term 
        # (we could have also chose w_0= average y value)
        y_train_mean = np.mean(y_train)
        y_train = y_train - y_train_mean
        y_test = y_test - y_train_mean
        # scaling the data matrix
        scaler = preprocessing.StandardScaler().fit(X_train)
        X_train = scaler.transform(X_train)
        X_test = scaler.transform(X_test)
        # determine the training error and the test error
        if type_test == 'ridge':
            w = get_coeff_ridge_normaleq(X_train=X_train,y_train=y_train,alpha=alpha)        
        if type_test == "linear":
            w = get_coeff_lin_reg(X_train=X_train,y_train=y_train)
            w_lin  = w_lin + w
            # print('W is' , w)
        y_pred_lin_alg = np.matmul(X_train, w)
        E_train_in = sum(np.square(y_train - y_pred_lin_alg))/X_train.shape[0]
        if type_test == 'ridge':
            w = get_coeff_ridge_normaleq(X_train=X_train,y_train=y_train,alpha=alpha)
            w_ridge = w_ridge + w
        if type_test == "linear":
            w = get_coeff_lin_reg(X_train = X_train, y_train=y_train )
            w_lin = w_lin + w
        y_test_pred_lin_alg = np.matmul(X_test, w)
        # print( 'Y Test is as', y_test_pred_lin_alg[0:5])
        # print('y_test is ' , y_test[0:5])
        # print('Shape of y_test is ', y_test.shape[0])
        E_test_in = sum((y_test - y_test_pred_lin_alg)*(y_test - y_test_pred_lin_alg))/X_train.shape[0]
        # print('E_test Val now is ' , E_test_in)
        E_val_test_now = None
        E_val_train_now = None
        if type_test.lower() == 'linear':
            E_val_test_now = E_test_in 
        elif type_test.lower() == 'ridge':
            w_square = np.square(w)            
            E_val_test_now = E_test_in + alpha* sum(w_square)
            # print('E_Test_in is  ' , E_test_in)
            # print('Penalty Term is ' , alpha* sum(w_square))
            # print('Final E_test is ' , E_val_test_now)
        if type_test.lower() == 'linear':
            E_val_train_now = E_train_in
        elif type_test.lower() == 'ridge':
            w_square = np.square(w)
            E_val_train_now = E_train_in + alpha*sum(w_square)
        E_train_list.append(E_val_train_now)
        E_test_list.append(E_val_test_now)
    E_val_train = sum(E_train_list)/len(E_train_list)
    # print('Alpha = ' ,alpha)
    E_val_test = sum(E_test_list)/len(E_test_list)
    # print('E_val_test ', E_val_test)
        ##############
    if type_test.lower() == 'linear':
        # print('W lin is' , w_lin/k)
        return w_lin/k , E_val_test[0], E_val_train[0]
    elif type_test.lower() == "ridge":
        # print('W ridge is' , w_ridge/k)
        return  w_ridge/k , E_val_test[0], E_val_train[0]

#%%

ones_matrix = np.ones(shape=(X.shape[0],1))
X_concat = np.concatenate((ones_matrix,X),axis=1)
pseudoinv_X = np.linalg.pinv(X_concat)
y_lin_reg = np.zeros(shape=(y.shape[0],1))
# print(pseudoinv_X)

E_lin_train_list = list()
E_lin_test_list = list()
# Splitting the Data into Test and Train parts
df_X_train, df_X_test, y_train, y_test = train_test_split(X,y)
scaler = preprocessing.StandardScaler().fit(df_X_train)
X_train = scaler.transform(df_X_train)
X_test = scaler.transform(df_X_test)
w_lin, E_lin_test , E_lin_train = k_fold_cross_validation(k=3,X=X,y=y,type_test="linear")
y_train_lin_reg = np.matmul(X_train, w_lin)
# print('Linear regression output is ', y_train_lin_reg[0:5] )
print('Error for linear regression is ' ,E_lin_test)
# print('W for linear regression is' ,w_lin)
y_train_mean = np.mean(y_train)
print(' In Linear Regression, the average of the Scores of training outputs is ' , sum(y_train_lin_reg +y_train_mean )/len(y_train_lin_reg))

# Linear Regression on Test Data 
y_test_lin_reg = np.matmul(X_test, w_lin)

print('In Linear Regression, the average of the Scores of testing outputs is ' , sum(y_test_lin_reg + y_train_mean)/len(y_test_lin_reg))
# print('W lin is ' , w_lin )

# Performing Ridge Regression
E_train_list = list()
E_test_list = list()
w_ridge_list = list()
alpha_list = np.logspace(1, 7, num=13)
for alpha in alpha_list:
    # print('Alpha is ' ,alpha )
    w_ridge, E_test,E_train = k_fold_cross_validation(k=5,X=X,y=y,alpha=alpha,type_test="ridge")
    E_train_list.append(E_train)
    E_test_list.append(E_test)
    # print('E_test is ' , E_test )
    w_ridge_list.append(w_ridge )

E_test_min = min(E_test_list)
print('E_test for Ridge Regression is  ' , E_test_min)
arg = E_test_list.index(E_test_min)
# print('Arg is ' , arg )
w_ridge = w_ridge_list[arg]
y_test_ridge_reg = np.matmul(X_test, w_ridge)
# print('w_ridge is ' , w_ridge)

y_train_ridge_reg = np.matmul(X_train, w_ridge)
print(' In Ridge Regression, the average of Scores of testing outputs is ' , sum(y_test_ridge_reg + y_train_mean)/len(y_test_ridge_reg))
print('In Ridge Regression, the average of Scores of training outputs is ' , sum(y_train_ridge_reg + y_train_mean)/len(y_train_lin_reg))



# Using Polynomial Regression 

df_X_2_train, df_X_2_test, y_2_train, y_2_test = train_test_split(X_2,y_2)
scaler = preprocessing.StandardScaler().fit(df_X_2_train)
X_2_train = scaler.transform(df_X_2_train)
X_2_test = scaler.transform(df_X_2_test)
w_lin_poly, E_lin_test_poly , E_lin_train_poly = k_fold_cross_validation(k=10,X=X_2,y=y_2,type_test="linear")
y_train_poly_lin_reg = np.matmul(X_2_train, w_lin_poly)
print('Linear regression output is ', y_train_poly_lin_reg[0:5] )
print('Error for polynomial regression with Test data is ' ,E_lin_test_poly)
# print('W for  polynomial regression is' ,w_lin_poly)
y_train_poly_mean = np.mean(y_2_train)
print(' In Polynomial Regression, the average of the Scores of training outputs is ' , sum(y_train_poly_mean +y_train_poly_lin_reg)/len(y_train_poly_lin_reg))

# Performing Ridge Regression
E_train_poly_list = list()
E_test_poly_list = list()
w_ridge_poly_list = list()
alpha_list = np.logspace(1, 7, num=13)
for alpha in alpha_list:
    # print('Alpha is ' ,alpha )
    w_ridge_poly, E_test_poly,E_train_poly = k_fold_cross_validation(k=10,X=X_2,y=y_2,alpha=alpha,type_test="ridge")
    E_train_poly_list.append(E_train_poly)
    E_test_poly_list.append(E_test_poly)
    # print('E_test is ' , E_test )
    w_ridge_poly_list.append(w_ridge_poly )

E_test_poly_min = min(E_test_poly_list)
print('Error for polynomial ridge regression is  ' , E_test_poly_min)
arg = E_test_poly_list.index(E_test_poly_min)
# print('Arg is ' , arg )
w_ridge_poly = w_ridge_poly_list[arg]
y_test_ridge_reg_poly = np.matmul(X_2_test, w_ridge_poly)
# print('w_ridge is ' , w_ridge)

y_train_ridge_reg_poly = np.matmul(X_2_train, w_ridge_poly)
print(' In Polynomial Ridge Regression, the average of Scores of testing outputs is ' , sum(y_test_ridge_reg_poly + y_train_poly_mean)/len(y_test_ridge_reg_poly))
print('In Polynomial Ridge Regression, the average of Scores of training outputs is ' , sum(y_train_ridge_reg_poly + y_train_poly_mean)/len(y_train_ridge_reg_poly))

# Error for polynomial linear regression is the least, thus choosing this model for computation

print('The error for regression using polynomial transformation is the least\n. Hence, I would use this model for prediction')

X_features = [[5], 
              [0.5],
              [2], 
              [0], 
              [4], 
              [8], 
              [4],
              [6], 
              [2],
              [2], 
              [2],
              [4],
              [5.5]]
X_features =np.array(X_features)
X_features =np.transpose(X_features)
# print('X_features ' , X_features)
print('Shape of X_features:' , X_features.shape)
scaler = preprocessing.StandardScaler().fit(X_features)
X_features_scale = scaler.transform(X_features)
poly = PolynomialFeatures(degree=2)
X_features_2 = poly.fit_transform(X_features_scale)
print('X_features_2 Shape' , X_features_2.shape)
y_pred_features =  np.matmul(X_features_2, w_lin_poly)
print('The output of the regression prediction model for the given input is:',y_pred_features +y_train_poly_mean)
