#%% md

# Logistic Regression with Ridge Regularisation

Importing packages and getting data.

Global variables declared here which can be set as parameters

#%%

from sklearn.datasets import load_breast_cancer # taking included data set from Sklearn http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html
from sklearn import preprocessing # preprossing is what we do with the data before we run the learning algorithm
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd

# Logistic Regression with Ridge Regularization is implemented in the code below
# The code in the lab experiment has been modified to incorporate ridge penalty. This has been made compulsory.
# Its results have been compared with the sklearn library  The true positives, false positives,
# true negatives and false negatives have been printed for both the methods
# The precision, recall, f1 score and accuracy have been calculated further for both the methods

# Global variables defined to set parameters

learning_rate = 0.1 # used in the lab experiment code while updating the
num_iters = 100 # The number of iteratins to run the gradient ascent algorithm
lamb = 0.001
penalty = "none"
print('The parameters set are:\n ' , "Learning Rate: " , learning_rate , "\nNumber of iterations: " ,num_iters , "\n Lambda: " , lamb , "\n Penalty: " , penalty) 


#%% md

# Breast Cancer Dataset

#%%

cancer = load_breast_cancer()
y = cancer.target
X = cancer.data

#%% md

Creating and setting up test and training data 

#%%

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)
scaler = preprocessing.StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
scaler_test = preprocessing.StandardScaler().fit(X_test)
X_test = scaler_test.transform(X_test)

# Step 1: Create a column vector of ones (i.e. a vector of shape N',1)
ones = np.ones(X_train.shape[0]).reshape((X_train.shape[0], 1))
# Step 2: Append a column of ones in the beginning of x_train
X_train = np.hstack((ones, X_train))

# Now do the same for the test data
# Step 1: Create a column vector of ones (i.e. a vector of shape N",1)
ones = np.ones(X_test.shape[0]).reshape((X_test.shape[0], 1))
# Stemp 2: Append a column of ones in the beginning of x_test
X_test = np.hstack((ones, X_test))


#%% md


Funtion to calculate sigmoid of a vector 

#%%

def sigmoid(z):
    import math
    # print(type(z))
    if str(type(z)).__contains__('numpy'):
        sigmoid_z = 1/(1+ np.exp(-z))
    else:
        sigmoid_z = 1/(1+ math.exp(-z))
    return sigmoid_z

#%% md

Initialize w

#%%

# Initialize parameters w

w = np.zeros((X_train.shape[1], 1))

#%% md


Function to calculate hypothesis of a vector

#%%

def hypothesis(X , w):
    matrix_mult = np.matmul(X,w)
    hypothesis_matrix = sigmoid(matrix_mult)
    return hypothesis_matrix


yhat = hypothesis(X_train, w)

#%% md


Function to calculate Log Likelihood between vectors X, y and w 

#%%

def log_likelihood(X , y , w ):
    log_likelihood_output = np.matmul(np.transpose(y),np.log(hypothesis(X,w)))   +\
                             np.matmul(np.transpose((1-y)),np.log(1 - hypothesis(X,w)))
    return log_likelihood_output

y_train = y_train.reshape(y_train.shape[0],1)


#%% md

Function to calculate Logistic Regression output on the dataset 

#%%

def Logistic_Regresion_Gradient_Ascent(X, y, learning_rate, num_iters, lamb=lamb):
    # For every 100 iterations, store the log_likelihood for the current w
    # Initializing log_likelihood to be an empty list
    log_likelihood_values = list()
    # Initialize w to be a zero vector of shape x_train.shape[1],1
    w = np.zeros((X.shape[1], 1))
    # Initialize N to the number of training examples
    N = X.shape[0]
    # Gradient Ascent formula is w = w_old + alpha/N*(sum(y(i) - h(x(i))*x(i))
    for i in range(0,num_iters):
        if (i % 1) == 0:
            # print('Log Likelihood is ' , log_likelihood(X,y,w)[0][0])
            log_likelihood_values.append(log_likelihood(X,y,w)[0][0])
        y = y.reshape(y.shape[0], 1)
        yhat = hypothesis(X, w)
        w = w + (np.matmul((np.transpose(X)), y -yhat) - 2*(lamb*w))*learning_rate/N
    return w, log_likelihood_values

w, log_likelihood_values = Logistic_Regresion_Gradient_Ascent(X_train, y_train, learning_rate, num_iters,lamb=lamb)

#%% md

Code to get parameters that detail on performance of the Logisitic Regression model

#%%

hypothesis_test = hypothesis(X_test, w)
y_pred = np.zeros(shape=(hypothesis_test.shape[0],1))
for i in range(0,hypothesis_test.shape[0]):
    if hypothesis_test[i] >= 0.5:
        y_pred[i] = 1
    else:
        y_pred[i] =0

# Finding TP, TN, FP and FN from y_pred and y_test
true_positives = 0
true_negatives = 0
false_positives = 0
false_negatives = 0

for i in range(0,y_pred.shape[0]):
    if y_test[i] == y_pred[i] and y_test[i] == 1:
        true_positives = true_positives + 1
    if y_test[i] == y_pred[i] and y_test[i] == 0:
        true_negatives = true_negatives + 1
    if y_test[i] != y_pred[i] and y_test[i] == 0:
        false_positives = false_positives + 1
    if y_test[i] != y_pred[i] and y_test[i] == 1:
        false_negatives = false_negatives + 1


# True Positives= Predicted true and were true
# True Negatives = Predicted false and were flase
# False Positives = Predicted true but were false
# False Negatives = Predicted false but were true
print('True Positives are: ' , true_positives)
print('True Negatives are: ', true_negatives )
print('False Positives are: ' , false_positives)
print('False Negatives are: ', false_negatives )

precision = true_positives/(true_positives+ false_positives)
print('The precision is ', precision)
recall = true_positives/(true_positives + false_negatives )
print('The recall is ' , recall)

f1_score = 2*(precision * recall)/(precision + recall)

print('The F1 score is ' , f1_score)

confusion_matrix = np.zeros(shape=(2,2))

confusion_matrix[0,0] = true_positives
confusion_matrix[0,1] = false_negatives
confusion_matrix[1,0] = false_positives
confusion_matrix[1,1] = true_negatives

print('The Confusion Matrix is: ', confusion_matrix[0:confusion_matrix.shape[1]] )

accuracy = (true_positives + true_negatives)/y_pred.shape[0]
print('The Accuracy using code from lab experiment is ' , accuracy*100 ," %")


#%% md

Code for Logistic Regression using SKlearn

#%%

from sklearn.linear_model import LogisticRegression
from sklearn import metrics

# All other parameters except C (ie  lambda) and penalty are not kept configurable as they are set to constant values
logreg = LogisticRegression(penalty=penalty,solver='sag', max_iter=num_iters,C=lamb,fit_intercept=False)
y_train = y_train.ravel()
logreg.fit(X_train, y_train)

y_pred_sklearn = logreg.predict(X_test)


#%% md


Code to calculate the parameters that detail on the Logistic Regression model performance using SKLearn

#%%

true_positives_sklearn = 0
true_negatives_sklearn = 0
false_positives_sklearn = 0
false_negatives_sklearn = 0

for i in range(0,y_pred.shape[0]):
    if y_test[i] == y_pred_sklearn[i] and y_test[i] == 1:
        true_positives_sklearn = true_positives_sklearn + 1
    if y_test[i] == y_pred_sklearn[i] and y_test[i] == 0:
        true_negatives_sklearn = true_negatives_sklearn + 1
    if y_test[i] != y_pred_sklearn[i] and y_test[i] == 0:
        false_positives_sklearn = false_positives_sklearn + 1
    if y_test[i] != y_pred_sklearn[i] and y_test[i] == 1:
        false_negatives_sklearn = false_negatives_sklearn + 1


print('True Positives are: ' , true_positives_sklearn)
print('True Negatives are: ', true_negatives_sklearn )
print('False Positives are: ' , false_positives_sklearn)
print('False Negatives are: ', false_negatives_sklearn )

precision_sklearn = true_positives_sklearn/(true_positives_sklearn+ false_positives_sklearn)
print('The precision is ', precision_sklearn)
recall_sklearn = true_positives_sklearn/(true_positives_sklearn + false_negatives_sklearn )
print('The recall is ' , recall_sklearn)

f1_score_sklearn= 2*(precision_sklearn * recall_sklearn)/(precision_sklearn + recall_sklearn)

print('The F1 score is ' , f1_score_sklearn)

confusion_matrix_sklearn = np.zeros(shape=(2,2))

confusion_matrix_sklearn[0,0] = true_positives_sklearn
confusion_matrix_sklearn[0,1] = false_negatives_sklearn
confusion_matrix_sklearn[1,0] = false_positives_sklearn
confusion_matrix_sklearn[1,1] = true_negatives_sklearn

print('The Confusion Matrix is: ', confusion_matrix_sklearn[0:confusion_matrix_sklearn.shape[1]] )

accuracy_sklearn = (true_positives_sklearn + true_negatives_sklearn)/y_pred.shape[0]
print('The Accuracy using sklearn library is ' , accuracy_sklearn*100 , "%" )

#%% md

# Salmon Dataset

#%%

#Code that was used to analyze Salmon data, commented here for now
df = pd.read_csv('Salmon_dataset.csv', delimiter=',')
df['Origin']=df.Origin.map({'Alaskan':0, 'Canadian':1})
X = df[['Gender','Freshwater','Marine']].to_numpy() #selecting the two of the features `Freshwater' and 'Marine'
y = df['Origin'].to_numpy()


#%%

X_train, X_test, y_train, y_test = train_test_split(X, y,shuffle=None)
scaler = preprocessing.StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
scaler_test = preprocessing.StandardScaler().fit(X_test)
X_test = scaler_test.transform(X_test)

# Step 1: Create a column vector of ones (i.e. a vector of shape N',1)
ones = np.ones(X_train.shape[0]).reshape((X_train.shape[0], 1))
# Step 2: Append a column of ones in the beginning of x_train
X_train = np.hstack((ones, X_train))

# Now do the same for the test data
# Step 1: Create a column vector of ones (i.e. a vector of shape N",1)
ones = np.ones(X_test.shape[0]).reshape((X_test.shape[0], 1))
# Stemp 2: Append a column of ones in the beginning of x_test
X_test = np.hstack((ones, X_test))


#%%

w = np.zeros((X_train.shape[1], 1))

#%%

yhat = hypothesis(X_train, w)

#%%

y_train = y_train.reshape(y_train.shape[0],1)

#%%

# Initialize parameters w

w = np.zeros((X_train.shape[1], 1))

#%%

w, log_likelihood_values = Logistic_Regresion_Gradient_Ascent(X_train, y_train, learning_rate, num_iters,lamb=lamb)

#%%

hypothesis_test = hypothesis(X_test, w)
y_pred = np.zeros(shape=(hypothesis_test.shape[0],1))
for i in range(0,hypothesis_test.shape[0]):
    if hypothesis_test[i] >= 0.5:
        y_pred[i] = 1
    else:
        y_pred[i] =0

# Finding TP, TN, FP and FN from y_pred and y_test
true_positives = 0
true_negatives = 0
false_positives = 0
false_negatives = 0

for i in range(0,y_pred.shape[0]):
    if y_test[i] == y_pred[i] and y_test[i] == 1:
        true_positives = true_positives + 1
    if y_test[i] == y_pred[i] and y_test[i] == 0:
        true_negatives = true_negatives + 1
    if y_test[i] != y_pred[i] and y_test[i] == 0:
        false_positives = false_positives + 1
    if y_test[i] != y_pred[i] and y_test[i] == 1:
        false_negatives = false_negatives + 1


# True Positives= Predicted true and were true
# True Negatives = Predicted false and were flase
# False Positives = Predicted true but were false
# False Negatives = Predicted false but were true
print('True Positives are: ' , true_positives)
print('True Negatives are: ', true_negatives )
print('False Positives are: ' , false_positives)
print('False Negatives are: ', false_negatives )

precision = true_positives/(true_positives+ false_positives)
print('The precision is ', precision)
recall = true_positives/(true_positives + false_negatives )
print('The recall is ' , recall)

f1_score = 2*(precision * recall)/(precision + recall)

print('The F1 score is ' , f1_score)

confusion_matrix = np.zeros(shape=(2,2))

confusion_matrix[0,0] = true_positives
confusion_matrix[0,1] = false_negatives
confusion_matrix[1,0] = false_positives
confusion_matrix[1,1] = true_negatives

print('The Confusion Matrix is: ', confusion_matrix[0:confusion_matrix.shape[1]] )

accuracy = (true_positives + true_negatives)/y_pred.shape[0]
print('The Accuracy using code from lab experiment is ' , accuracy*100 ," %")


#%%

from sklearn.linear_model import LogisticRegression
from sklearn import metrics

# All other parameters except C (ie  lambda) and penalty are not kept configurable as they are set to constant values
logreg = LogisticRegression(penalty=penalty,solver='sag', max_iter=num_iters,C=lamb,fit_intercept=False)
y_train = y_train.ravel()
logreg.fit(X_train, y_train)

y_pred_sklearn = logreg.predict(X_test)


#%%

true_positives_sklearn = 0
true_negatives_sklearn = 0
false_positives_sklearn = 0
false_negatives_sklearn = 0

for i in range(0,y_pred.shape[0]):
    if y_test[i] == y_pred_sklearn[i] and y_test[i] == 1:
        true_positives_sklearn = true_positives_sklearn + 1
    if y_test[i] == y_pred_sklearn[i] and y_test[i] == 0:
        true_negatives_sklearn = true_negatives_sklearn + 1
    if y_test[i] != y_pred_sklearn[i] and y_test[i] == 0:
        false_positives_sklearn = false_positives_sklearn + 1
    if y_test[i] != y_pred_sklearn[i] and y_test[i] == 1:
        false_negatives_sklearn = false_negatives_sklearn + 1


print('True Positives are: ' , true_positives_sklearn)
print('True Negatives are: ', true_negatives_sklearn )
print('False Positives are: ' , false_positives_sklearn)
print('False Negatives are: ', false_negatives_sklearn )

precision_sklearn = true_positives_sklearn/(true_positives_sklearn+ false_positives_sklearn)
print('The precision is ', precision_sklearn)
recall_sklearn = true_positives_sklearn/(true_positives_sklearn + false_negatives_sklearn )
print('The recall is ' , recall_sklearn)

f1_score_sklearn= 2*(precision_sklearn * recall_sklearn)/(precision_sklearn + recall_sklearn)

print('The F1 score is ' , f1_score_sklearn)

confusion_matrix_sklearn = np.zeros(shape=(2,2))

confusion_matrix_sklearn[0,0] = true_positives_sklearn
confusion_matrix_sklearn[0,1] = false_negatives_sklearn
confusion_matrix_sklearn[1,0] = false_positives_sklearn
confusion_matrix_sklearn[1,1] = true_negatives_sklearn

print('The Confusion Matrix is: ', confusion_matrix_sklearn[0:confusion_matrix_sklearn.shape[1]] )

accuracy_sklearn = (true_positives_sklearn + true_negatives_sklearn)/y_pred.shape[0]
print('The Accuracy using sklearn library is ' , accuracy_sklearn*100 , "%" )

#%%



#%%



#%%


